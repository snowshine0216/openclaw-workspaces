# Session: 2026-02-21 22:33:41 UTC

- **Session Key**: agent:information-collector:main
- **Session ID**: e24b4a10-c1f7-42f3-bb63-6267bd482dea
- **Source**: feishu

## Conversation Summary

assistant: [{model} | {think}] âœ… New session started Â· model: minimax-portal/MiniMax-M2.5
user: System: [2026-02-20 23:53:32 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: Add to knowledge base
System: [2026-02-21 00:11:11 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: /new

A new session was started via /new or /reset. Greet the user in your configured persona, if one is provided. Be yourself - use your defined voice, mannerisms, and mood. Keep it to 1-3 sentences and ask what they want to do. If the runtime model differs from default_model in the system prompt, mention the default model. Do not mention internal steps, files, tools, or reasoning.
assistant: <final>

Hey Snow! ðŸ”­ Fresh session, ready to roll. What do you need â€” research, updates, or something else?

</final>
user: System: [2026-02-21 00:11:30 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: /new
System: [2026-02-21 00:12:59 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518

Conversation info (untrusted metadata):
```json
{
  "message_id": "om_x100b56bbc679a0a0b2dc6c38dba5cb5",
  "sender": "ou_ac0664ae2ad545cdbfa0ffe2c56a9328"
}
```

https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518
assistant: ## ðŸ”¬ AI Agent Testing in Production

### Key Takeaways

- **Testing pyramid for AI agents**: Unit tests â†’ Integration tests â†’ LLM-as-judge evaluations â†’ A/B testing â†’ Production monitoring
- **Mock the LLM**: For unit tests, mock LLM responses to avoid non-determinism, cost, and latency issues
- **Test data categories**: Happy path, edge cases, error cases, and adversarial inputs
- **LLM-as-judge**: Use another LLM to evaluate responses at scale (relevance, accuracy, completeness, clarity, helpfulness)
- **Consistency testing**: Run same query multiple times, measure variance via embedding similarity
- **CI/CD integration**: Automated testing gates with quality thresholds before deployment

### Actions to Take

- **High**: Build test dataset with all 4 categories (happy path, edge, error, adversarial)
- **High**: Implement LLM-as-judge evaluation early â€” scales better than human eval
- **Medium**: Add consistency testing to catch non-deterministic behavior
- **Medium**: Set up production monitoring with deviation alerts

### Areas for Deeper Dive

1. **LangGraph state management testing patterns**
2. **Specific tools for agent evaluation** (LangSmith, Promptfoo, etc.)
3. **Adversarial prompt injection testing strategies**

---

Want me to save this to the knowledge base? Topics: `ai-research` or `ai-tools`?

Let me also grab some related research:

assistant: ### Related Research

- **State of AI Agents 2026** (Databricks report) â€” Enterprises shifting from chatbots to agentic systems with multi-step workflows â†’ [Link](https://lovelytics.com/post/state-of-ai-agents-2026-lessons-on-governance-evaluation-and-scale/)
- **Top 5 AI Testing Trends for 2026** â€” Autonomous testing + AI agents joining QA teams â†’ [Link](https://www.parasoft.com/blog/annual-software-testing-trends/)
- **Agent Testing Complete Guide** â€” Validating decision sequences, tool accuracy, and context retention â†’ [Link](https://www.openlayer.com/blog/post/agent-testing-complete-guide-validating-ai-systems)
user: System: [2026-02-21 00:13:20 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518
System: [2026-02-21 00:14:05 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: Review https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518

Conversation info (untrusted metadata):
```json
{
  "message_id": "om_x100b56bbc21f64a0b2d1a548ec981dd",
  "sender": "ou_ac0664ae2ad545cdbfa0ffe2c56a9328"
}
```

Review https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518
assistant: I just analyzed this article above! Here's my review:

---

## ðŸ“– Article Review: "Your AI Agent Works in Dev. Why Does It Fail in Production?"

**Author:** @v31u (Jan 11, 2026)  
**Length:** 8 min read

### Summary
A practical guide on testing AI agents â€” from unit tests to production monitoring. Fills a real gap since traditional software testing doesn't apply well to LLM-based systems.

### Strengths
- **Actionable code examples** â€” mocking LLMs, testing state management, tool selection
- **Comprehensive framework** â€” covers the full testing pyramid
- **Real metrics** â€” completion rate, response time, error rate, consistency
- **CI/CD integration** â€” GitHub Actions pipeline example

### Weaknesses
- Surface-level on **LLM-as-judge** â€” doesn't dive into prompt engineering for evaluators
- Missing discussion on **cost management** for evaluation at scale
- No mention of **existing tools** (LangSmith, Promptfoo, Helicone)

### Verdict
Solid intermediate resource. Good for teams who've built their first agent and are now hitting the "why does this work in dev?" wall. Not for beginners or advanced practitioners.

**Rating:** 7/10 â€” Practical but safe. Could go deeper on eval metrics and tooling.

---

Want me to save this to the knowledge base?
user: System: [2026-02-21 00:14:25 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: Review https://medium.com/@v31u/your-ai-agent-works-in-dev-why-does-it-fail-in-production-e591adfe6518
System: [2026-02-21 00:15:09 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: Dig more for. Key takeaways and action to take

Conversation info (untrusted metadata):
```json
{
  "message_id": "om_x100b56bbde065ca4b2e106ae11542e6",
  "sender": "ou_ac0664ae2ad545cdbfa0ffe2c56a9328"
}
```

Dig more for. Key takeaways and action to take
assistant: ## ðŸ”‘ Key Takeaways

| # | Insight | Why It Matters |
|---|---------|----------------|
| 1 | **Mock LLMs in unit tests** | Real API calls are slow, costly, and non-deterministic â€” kills test reliability |
| 2 | **4 categories of test data** â€” Happy path, edge cases, error cases, adversarial | Covers real-world failure modes |
| 3 | **LLM-as-judge replaces human eval** | Scales evaluation without burning human time |
| 4 | **Consistency testing = embedding similarity** | Detects non-determinism (min_similarity > 0.8 = stable) |
| 5 | **Metrics beyond pass/fail** â€” completion rate, p95 latency, error rate | Real performance visibility |
| 6 | **A/B test with p-value < 0.05** | Data-driven decisions, not gut feelings |
| 7 | **Production monitoring with deviation alerts** | Catch regressions before users complain |

---

## ðŸŽ¯ Actionable Next Steps

| Priority | Action | How |
|----------|--------|-----|
| **P0** | Mock LLM in your test suite | Use `unittest.mock.AsyncMock` â€” article has code snippet |
| **P0** | Build test dataset (4 categories) | Start with 10 cases per category |
| **P1** | Add LLM-as-judge eval | Use GPT-4o with temp=0, score on 5 criteria |
| **P1** | Track metrics (completion_rate, latency) | Implement `AgentMetrics` dataclass |
| **P2** | Set up CI/CD gate | Run eval before deploy, fail if avg score < 0.8 |
| **P2** | A/B test prompts/models | Compare with statistical significance |

---

Want me to save this to knowledge base (`ai-tools`)?
user: System: [2026-02-21 00:15:29 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: Dig more for. Key takeaways and action to take
System: [2026-02-21 00:16:02 UTC] Feishu[information-collector] DM from ou_ac0664ae2ad545cdbfa0ffe2c56a9328: yes

Conversation info (untrusted metadata):
```json
{
  "message_id": "om_x100b56bbdad0d4a0b2d2a269b131f27",
  "sender": "ou_ac0664ae2ad545cdbfa0ffe2c56a9328"
}
```

yes
assistant: Done! Saved to:

ðŸ“ `projects/knowledge-base/ai-tools/2026-02-21-ai-agent-testing-production.md`

Updated MEMORY.md with the reference.
assistant: ## ðŸ”­ AI Intel Digest â€” Feb 21, 2026

**No new arXiv papers** â€” same 5 cs.AI submissions from Feb 19.

### GitHub Trending (AI/ML)

**1. microsoft/agent-framework** â€” Framework for building, orchestrating & deploying AI agents & multi-agent workflows (Py + .NET)

**2. google-research/timesfm** â€” Pretrained time-series foundation model for forecasting

**3. trycua/cua** â€” Open-source infra for Computer-Use Agents. Sandboxes, SDKs & benchmarks to train/eval AI agents that control desktops

**4. Fosowl/agenticSeek** â€” Fully local Manus AI. No APIs, no $200/mo bills. Autonomous agent that thinks, browses & codes

**5. huggingface/skills** â€” AI skills framework

**6. usestrix/strix** â€” Open-source AI hackers to find & fix app vulnerabilities

**7. roboflow/supervision** â€” Reusable computer vision tools

**8. databricks-solutions/ai-dev-kit** â€” Databricks toolkit for coding agents

---

That agenticSeek repo is spicy â€” fully local Manus alternative without API costs? That's gonna hit a nerve. And Microsoft's agent framework going official is a big signal.

ðŸ”­
